{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1afb3a4-4ea5-45ea-8484-56494275599f",
   "metadata": {},
   "source": [
    "# 모델 매개변수 최적화\n",
    "- 데이터에 매개변수를 최적화하여 모델을 학습하고, 검증하고, 테스트 하기\n",
    "- 모델을 학습하는 과정은 반복적인 과정을 거친다. (epoch라 부른다.)\n",
    "- 반복 단계에서 모델은 출력을 추측하고, 추측과 정답 사이의 오류(loss)를 계산한다. \n",
    "- 매개변수에 대한 오류의 도함수(derivative)를 수집한 뒤, 경사하강법을 사용하여 이 parameter들을 최적화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "392ebdb9-0094-45e0-a776-aee3f960b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"이전 step에서 했던 코드들\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, drop_last=True)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "\"\"\"이전 step에서 했던 코드들\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240eb5c-7798-4601-b4ca-a02f6e52b1a3",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "- 하이퍼파라미터는 모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수이다.\n",
    "- 서로 다른 하이퍼파라미터 값은 모델 학습과 수렴율(converence rate)에 영향을 미칠 수 있다.\n",
    "- 하이퍼파라미터 정의\n",
    "    - 에폭(epoch) 수 : 데이터 셋을 반복하는 횟수\n",
    "    - 배치 크기 (batch size) : 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n",
    "    - 학습률 (learning rate) : 각 배치/epoch에서 모델의 매개변수를 조절하는 비율. 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f9ba7b5-6c7b-4d50-abe1-212b845823e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d789bfa4-f5a6-424c-add8-a3d701b280ab",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710728f-aef4-4ba8-8a02-f8d59452c4b7",
   "metadata": {},
   "source": [
    "### 최적화 단계\n",
    "- 하이퍼파라미터를 설정한 뒤에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있다. (최적화 단계의 각 반복(iteration)을 `에폭(epoch)`이라고 부른다.)\n",
    "- 하나의 에폭은 다음 두 부분으로 구성된다.\n",
    "    1. 학습 단계 : train data set(학습용 데이터셋)을 반복(iterate)하고 최적의 매개변수로 수렴한다.\n",
    "    2. 검증/테스트 단계 : 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복(iterate)한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376aeab9-da2b-4e91-89d8-35e100e58544",
   "metadata": {},
   "source": [
    "#### 손실 함수(loss function)\n",
    "- 학습용 데이터를 제공하면, 학습되지 않은 신경망은 정답을 제공하지 않을 확률이 높다.\n",
    "- 손실 함수(loss fuction)는 획득한 결과와 실제 값 사이의 틀린 정도를 측정하며, 학습중에 이 값을 최소화하려고 한다. (label과 pred를 비교하여 loss를 계산)\n",
    "- 모델의 출렬 logit을 손실 함수에 전달하여 logit을 정규화하고 예측 오류를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e27a61-deee-40a9-8aee-ea439bffdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ad699-5399-4cb2-88c6-333d6a386cd0",
   "metadata": {},
   "source": [
    "#### 옵티마이저 (Optimizer)\n",
    "- 최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수(weight, bias)를 조정하는 과정이다.\n",
    "- 최적화 알고리즘은 이 과정이 수행되는 방식을 정의\n",
    "    - 최적화 알고리즘에는 많은 종류가 있다. (이 경우 SGD 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b37e39d-dd86-444d-9d91-81c38b0e7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"학습하려는 모델의 매개변수와 학습률(learning rate), 하이퍼파라미터를 등록하여 옵티마이저를 초기화한다.\"\"\"\n",
    "opimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3150b01-939c-4ddc-9e51-7f5e4ed98a22",
   "metadata": {},
   "source": [
    "#### 학습 단계에서 최적화는 세 단계로 이루어진다.\n",
    "\n",
    "1. `optimizer.zero_grad()`를 호출하여 `모델 매개변수의 변화도를 재설정`한다.\n",
    "    - 기본적으로 변화도는 더해지기 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정\n",
    "    \n",
    "2. `loss.backward()`를 호출하여 `예측 손실을 역전파`한다. \n",
    "    - pytorch는 각 매개변수에 대한 손실의 변화도를 저장한다.\n",
    "\n",
    "3. 변화도를 계산한 뒤에는 `optimizer.step()`을 호출하여 `역전파 단계에서 수집된 변화도로 매개변수를 조정`한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f4c57-147f-41c9-8cd1-359d9b3c869d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465eb9a6-7188-4b95-be4e-6ff170d3db3c",
   "metadata": {},
   "source": [
    "#### 전체 구현\n",
    "- 최적화 코드를 반복하여 수행 : train_loop\n",
    "- 테스트 데이터로 모델의 성능을 측정 : test_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee85d2b4-daa0-4aba-bf23-f5a791c19f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        ## 예측 (prediction)과 손실(loss) 계산\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y) # 손실값 계산\n",
    "        \n",
    "        ## 역전파\n",
    "        optimizer.zero_grad() # 모델 매개변수의 변화도 재설정\n",
    "        loss.backward() # 예측 손실 역전파\n",
    "        optimizer.step() # 역전파 단계에서 수집된 변화도로 매개변수 조정\n",
    "        \n",
    "        if batch%100==0:\n",
    "            loss, current = loss.item(), batch*len(X) # 100번마다 batch별 loss 값 확인\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21f7e633-1c12-4f29-865e-c6a1ec52abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset) # 전체 데이터 개수\n",
    "    num_batches = len(dataloader) # batch 개수\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(dim = 1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches # batch 별 평균 loss\n",
    "    correct /= size # 전체 test data 중에서 맞은 개수 확인\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cb0e515-4b7f-4c4c-b5bb-d221408f5786",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.310904  [    0/60000]\n",
      "loss: 2.271514  [ 6400/60000]\n",
      "loss: 2.216685  [12800/60000]\n",
      "loss: 2.191461  [19200/60000]\n",
      "loss: 2.123796  [25600/60000]\n",
      "loss: 2.024082  [32000/60000]\n",
      "loss: 1.986719  [38400/60000]\n",
      "loss: 1.823268  [44800/60000]\n",
      "loss: 1.757781  [51200/60000]\n",
      "loss: 1.577937  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 1.548143 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.617948  [    0/60000]\n",
      "loss: 1.510161  [ 6400/60000]\n",
      "loss: 1.290626  [12800/60000]\n",
      "loss: 1.324711  [19200/60000]\n",
      "loss: 1.146088  [25600/60000]\n",
      "loss: 1.139155  [32000/60000]\n",
      "loss: 1.138788  [38400/60000]\n",
      "loss: 1.039635  [44800/60000]\n",
      "loss: 1.056428  [51200/60000]\n",
      "loss: 0.972953  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.976325 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.033526  [    0/60000]\n",
      "loss: 1.039214  [ 6400/60000]\n",
      "loss: 0.820355  [12800/60000]\n",
      "loss: 0.973472  [19200/60000]\n",
      "loss: 0.821621  [25600/60000]\n",
      "loss: 0.847630  [32000/60000]\n",
      "loss: 0.920152  [38400/60000]\n",
      "loss: 0.844581  [44800/60000]\n",
      "loss: 0.856413  [51200/60000]\n",
      "loss: 0.821667  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.816559 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.825883  [    0/60000]\n",
      "loss: 0.891657  [ 6400/60000]\n",
      "loss: 0.651888  [12800/60000]\n",
      "loss: 0.849380  [19200/60000]\n",
      "loss: 0.721171  [25600/60000]\n",
      "loss: 0.729071  [32000/60000]\n",
      "loss: 0.827658  [38400/60000]\n",
      "loss: 0.771783  [44800/60000]\n",
      "loss: 0.768735  [51200/60000]\n",
      "loss: 0.746259  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.738708 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.712170  [    0/60000]\n",
      "loss: 0.803196  [ 6400/60000]\n",
      "loss: 0.562863  [12800/60000]\n",
      "loss: 0.783511  [19200/60000]\n",
      "loss: 0.668889  [25600/60000]\n",
      "loss: 0.665864  [32000/60000]\n",
      "loss: 0.761120  [38400/60000]\n",
      "loss: 0.727322  [44800/60000]\n",
      "loss: 0.715506  [51200/60000]\n",
      "loss: 0.692754  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.685375 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.634901  [    0/60000]\n",
      "loss: 0.735446  [ 6400/60000]\n",
      "loss: 0.504824  [12800/60000]\n",
      "loss: 0.738707  [19200/60000]\n",
      "loss: 0.633780  [25600/60000]\n",
      "loss: 0.626077  [32000/60000]\n",
      "loss: 0.706065  [38400/60000]\n",
      "loss: 0.695185  [44800/60000]\n",
      "loss: 0.679746  [51200/60000]\n",
      "loss: 0.649661  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.2%, Avg loss: 0.644266 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.578137  [    0/60000]\n",
      "loss: 0.682445  [ 6400/60000]\n",
      "loss: 0.463191  [12800/60000]\n",
      "loss: 0.703898  [19200/60000]\n",
      "loss: 0.607876  [25600/60000]\n",
      "loss: 0.599054  [32000/60000]\n",
      "loss: 0.660012  [38400/60000]\n",
      "loss: 0.673235  [44800/60000]\n",
      "loss: 0.656484  [51200/60000]\n",
      "loss: 0.613051  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.5%, Avg loss: 0.611997 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.534485  [    0/60000]\n",
      "loss: 0.641040  [ 6400/60000]\n",
      "loss: 0.432025  [12800/60000]\n",
      "loss: 0.675362  [19200/60000]\n",
      "loss: 0.586551  [25600/60000]\n",
      "loss: 0.579060  [32000/60000]\n",
      "loss: 0.622640  [38400/60000]\n",
      "loss: 0.660252  [44800/60000]\n",
      "loss: 0.642280  [51200/60000]\n",
      "loss: 0.581231  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.586536 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.499580  [    0/60000]\n",
      "loss: 0.608617  [ 6400/60000]\n",
      "loss: 0.407472  [12800/60000]\n",
      "loss: 0.650946  [19200/60000]\n",
      "loss: 0.567225  [25600/60000]\n",
      "loss: 0.562686  [32000/60000]\n",
      "loss: 0.592713  [38400/60000]\n",
      "loss: 0.653951  [44800/60000]\n",
      "loss: 0.633410  [51200/60000]\n",
      "loss: 0.552603  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.566259 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.470846  [    0/60000]\n",
      "loss: 0.582742  [ 6400/60000]\n",
      "loss: 0.387945  [12800/60000]\n",
      "loss: 0.630012  [19200/60000]\n",
      "loss: 0.548631  [25600/60000]\n",
      "loss: 0.547745  [32000/60000]\n",
      "loss: 0.568993  [38400/60000]\n",
      "loss: 0.652749  [44800/60000]\n",
      "loss: 0.627308  [51200/60000]\n",
      "loss: 0.527140  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.550047 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.446347  [    0/60000]\n",
      "loss: 0.561948  [ 6400/60000]\n",
      "loss: 0.371806  [12800/60000]\n",
      "loss: 0.611760  [19200/60000]\n",
      "loss: 0.530544  [25600/60000]\n",
      "loss: 0.533562  [32000/60000]\n",
      "loss: 0.549757  [38400/60000]\n",
      "loss: 0.653420  [44800/60000]\n",
      "loss: 0.622160  [51200/60000]\n",
      "loss: 0.504671  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.537006 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.425396  [    0/60000]\n",
      "loss: 0.545006  [ 6400/60000]\n",
      "loss: 0.358416  [12800/60000]\n",
      "loss: 0.595640  [19200/60000]\n",
      "loss: 0.513196  [25600/60000]\n",
      "loss: 0.520287  [32000/60000]\n",
      "loss: 0.533863  [38400/60000]\n",
      "loss: 0.654453  [44800/60000]\n",
      "loss: 0.617088  [51200/60000]\n",
      "loss: 0.485323  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.526324 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.407441  [    0/60000]\n",
      "loss: 0.531072  [ 6400/60000]\n",
      "loss: 0.346974  [12800/60000]\n",
      "loss: 0.581245  [19200/60000]\n",
      "loss: 0.497284  [25600/60000]\n",
      "loss: 0.508027  [32000/60000]\n",
      "loss: 0.520859  [38400/60000]\n",
      "loss: 0.654995  [44800/60000]\n",
      "loss: 0.611741  [51200/60000]\n",
      "loss: 0.468629  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.517413 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.391833  [    0/60000]\n",
      "loss: 0.519474  [ 6400/60000]\n",
      "loss: 0.337339  [12800/60000]\n",
      "loss: 0.568248  [19200/60000]\n",
      "loss: 0.482834  [25600/60000]\n",
      "loss: 0.496960  [32000/60000]\n",
      "loss: 0.509679  [38400/60000]\n",
      "loss: 0.654469  [44800/60000]\n",
      "loss: 0.606197  [51200/60000]\n",
      "loss: 0.454470  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.509842 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.378123  [    0/60000]\n",
      "loss: 0.509627  [ 6400/60000]\n",
      "loss: 0.328877  [12800/60000]\n",
      "loss: 0.556649  [19200/60000]\n",
      "loss: 0.469586  [25600/60000]\n",
      "loss: 0.486782  [32000/60000]\n",
      "loss: 0.499855  [38400/60000]\n",
      "loss: 0.652503  [44800/60000]\n",
      "loss: 0.600157  [51200/60000]\n",
      "loss: 0.442481  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.503269 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.365892  [    0/60000]\n",
      "loss: 0.501221  [ 6400/60000]\n",
      "loss: 0.321402  [12800/60000]\n",
      "loss: 0.546220  [19200/60000]\n",
      "loss: 0.457610  [25600/60000]\n",
      "loss: 0.477871  [32000/60000]\n",
      "loss: 0.491127  [38400/60000]\n",
      "loss: 0.649079  [44800/60000]\n",
      "loss: 0.594422  [51200/60000]\n",
      "loss: 0.432717  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.497476 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.355148  [    0/60000]\n",
      "loss: 0.493777  [ 6400/60000]\n",
      "loss: 0.314910  [12800/60000]\n",
      "loss: 0.536901  [19200/60000]\n",
      "loss: 0.446916  [25600/60000]\n",
      "loss: 0.469711  [32000/60000]\n",
      "loss: 0.483483  [38400/60000]\n",
      "loss: 0.644760  [44800/60000]\n",
      "loss: 0.588506  [51200/60000]\n",
      "loss: 0.424390  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.492251 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.345718  [    0/60000]\n",
      "loss: 0.486980  [ 6400/60000]\n",
      "loss: 0.308954  [12800/60000]\n",
      "loss: 0.528775  [19200/60000]\n",
      "loss: 0.437602  [25600/60000]\n",
      "loss: 0.462385  [32000/60000]\n",
      "loss: 0.476456  [38400/60000]\n",
      "loss: 0.639650  [44800/60000]\n",
      "loss: 0.583183  [51200/60000]\n",
      "loss: 0.417480  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.487453 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.336985  [    0/60000]\n",
      "loss: 0.480499  [ 6400/60000]\n",
      "loss: 0.303044  [12800/60000]\n",
      "loss: 0.521217  [19200/60000]\n",
      "loss: 0.428815  [25600/60000]\n",
      "loss: 0.455969  [32000/60000]\n",
      "loss: 0.469965  [38400/60000]\n",
      "loss: 0.634676  [44800/60000]\n",
      "loss: 0.577627  [51200/60000]\n",
      "loss: 0.411662  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.483136 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.328996  [    0/60000]\n",
      "loss: 0.474122  [ 6400/60000]\n",
      "loss: 0.297985  [12800/60000]\n",
      "loss: 0.514633  [19200/60000]\n",
      "loss: 0.420516  [25600/60000]\n",
      "loss: 0.450415  [32000/60000]\n",
      "loss: 0.463836  [38400/60000]\n",
      "loss: 0.629398  [44800/60000]\n",
      "loss: 0.572401  [51200/60000]\n",
      "loss: 0.406554  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.479161 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.321633  [    0/60000]\n",
      "loss: 0.468491  [ 6400/60000]\n",
      "loss: 0.293250  [12800/60000]\n",
      "loss: 0.508606  [19200/60000]\n",
      "loss: 0.412947  [25600/60000]\n",
      "loss: 0.445573  [32000/60000]\n",
      "loss: 0.458212  [38400/60000]\n",
      "loss: 0.623940  [44800/60000]\n",
      "loss: 0.567121  [51200/60000]\n",
      "loss: 0.402192  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.475451 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.315028  [    0/60000]\n",
      "loss: 0.463138  [ 6400/60000]\n",
      "loss: 0.288963  [12800/60000]\n",
      "loss: 0.503057  [19200/60000]\n",
      "loss: 0.405865  [25600/60000]\n",
      "loss: 0.441487  [32000/60000]\n",
      "loss: 0.453084  [38400/60000]\n",
      "loss: 0.618433  [44800/60000]\n",
      "loss: 0.561920  [51200/60000]\n",
      "loss: 0.398432  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.471956 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.308970  [    0/60000]\n",
      "loss: 0.458222  [ 6400/60000]\n",
      "loss: 0.285076  [12800/60000]\n",
      "loss: 0.497882  [19200/60000]\n",
      "loss: 0.399458  [25600/60000]\n",
      "loss: 0.437523  [32000/60000]\n",
      "loss: 0.448291  [38400/60000]\n",
      "loss: 0.613081  [44800/60000]\n",
      "loss: 0.556756  [51200/60000]\n",
      "loss: 0.395520  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.468655 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.303470  [    0/60000]\n",
      "loss: 0.453525  [ 6400/60000]\n",
      "loss: 0.281483  [12800/60000]\n",
      "loss: 0.493413  [19200/60000]\n",
      "loss: 0.393465  [25600/60000]\n",
      "loss: 0.433806  [32000/60000]\n",
      "loss: 0.443633  [38400/60000]\n",
      "loss: 0.607791  [44800/60000]\n",
      "loss: 0.551732  [51200/60000]\n",
      "loss: 0.392880  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.465523 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.298371  [    0/60000]\n",
      "loss: 0.448932  [ 6400/60000]\n",
      "loss: 0.278128  [12800/60000]\n",
      "loss: 0.489366  [19200/60000]\n",
      "loss: 0.387784  [25600/60000]\n",
      "loss: 0.430456  [32000/60000]\n",
      "loss: 0.439399  [38400/60000]\n",
      "loss: 0.602557  [44800/60000]\n",
      "loss: 0.547361  [51200/60000]\n",
      "loss: 0.390498  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.462556 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.293903  [    0/60000]\n",
      "loss: 0.444621  [ 6400/60000]\n",
      "loss: 0.274906  [12800/60000]\n",
      "loss: 0.485391  [19200/60000]\n",
      "loss: 0.382231  [25600/60000]\n",
      "loss: 0.427184  [32000/60000]\n",
      "loss: 0.435273  [38400/60000]\n",
      "loss: 0.597635  [44800/60000]\n",
      "loss: 0.542702  [51200/60000]\n",
      "loss: 0.388324  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.459725 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.289723  [    0/60000]\n",
      "loss: 0.440563  [ 6400/60000]\n",
      "loss: 0.272012  [12800/60000]\n",
      "loss: 0.481448  [19200/60000]\n",
      "loss: 0.376765  [25600/60000]\n",
      "loss: 0.424548  [32000/60000]\n",
      "loss: 0.431445  [38400/60000]\n",
      "loss: 0.592770  [44800/60000]\n",
      "loss: 0.538345  [51200/60000]\n",
      "loss: 0.386290  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.457056 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.286117  [    0/60000]\n",
      "loss: 0.436534  [ 6400/60000]\n",
      "loss: 0.269502  [12800/60000]\n",
      "loss: 0.477872  [19200/60000]\n",
      "loss: 0.371543  [25600/60000]\n",
      "loss: 0.421968  [32000/60000]\n",
      "loss: 0.427635  [38400/60000]\n",
      "loss: 0.588162  [44800/60000]\n",
      "loss: 0.534111  [51200/60000]\n",
      "loss: 0.384471  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.454471 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.282663  [    0/60000]\n",
      "loss: 0.432739  [ 6400/60000]\n",
      "loss: 0.266975  [12800/60000]\n",
      "loss: 0.474388  [19200/60000]\n",
      "loss: 0.366626  [25600/60000]\n",
      "loss: 0.419285  [32000/60000]\n",
      "loss: 0.423841  [38400/60000]\n",
      "loss: 0.583839  [44800/60000]\n",
      "loss: 0.529834  [51200/60000]\n",
      "loss: 0.382730  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.451965 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.279591  [    0/60000]\n",
      "loss: 0.429088  [ 6400/60000]\n",
      "loss: 0.264632  [12800/60000]\n",
      "loss: 0.470944  [19200/60000]\n",
      "loss: 0.361924  [25600/60000]\n",
      "loss: 0.416852  [32000/60000]\n",
      "loss: 0.420082  [38400/60000]\n",
      "loss: 0.579939  [44800/60000]\n",
      "loss: 0.525627  [51200/60000]\n",
      "loss: 0.381154  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.449574 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.276816  [    0/60000]\n",
      "loss: 0.425647  [ 6400/60000]\n",
      "loss: 0.262307  [12800/60000]\n",
      "loss: 0.467441  [19200/60000]\n",
      "loss: 0.357292  [25600/60000]\n",
      "loss: 0.414389  [32000/60000]\n",
      "loss: 0.416576  [38400/60000]\n",
      "loss: 0.576366  [44800/60000]\n",
      "loss: 0.521663  [51200/60000]\n",
      "loss: 0.379652  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.447224 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.274258  [    0/60000]\n",
      "loss: 0.422238  [ 6400/60000]\n",
      "loss: 0.260033  [12800/60000]\n",
      "loss: 0.463855  [19200/60000]\n",
      "loss: 0.352876  [25600/60000]\n",
      "loss: 0.411938  [32000/60000]\n",
      "loss: 0.412898  [38400/60000]\n",
      "loss: 0.572606  [44800/60000]\n",
      "loss: 0.517775  [51200/60000]\n",
      "loss: 0.378462  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.444983 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.271910  [    0/60000]\n",
      "loss: 0.419003  [ 6400/60000]\n",
      "loss: 0.258094  [12800/60000]\n",
      "loss: 0.460198  [19200/60000]\n",
      "loss: 0.348886  [25600/60000]\n",
      "loss: 0.409567  [32000/60000]\n",
      "loss: 0.409406  [38400/60000]\n",
      "loss: 0.569328  [44800/60000]\n",
      "loss: 0.513800  [51200/60000]\n",
      "loss: 0.377264  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.442818 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.269938  [    0/60000]\n",
      "loss: 0.416069  [ 6400/60000]\n",
      "loss: 0.256305  [12800/60000]\n",
      "loss: 0.456238  [19200/60000]\n",
      "loss: 0.345012  [25600/60000]\n",
      "loss: 0.407020  [32000/60000]\n",
      "loss: 0.405432  [38400/60000]\n",
      "loss: 0.566238  [44800/60000]\n",
      "loss: 0.510227  [51200/60000]\n",
      "loss: 0.376557  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.440617 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.268245  [    0/60000]\n",
      "loss: 0.413455  [ 6400/60000]\n",
      "loss: 0.254703  [12800/60000]\n",
      "loss: 0.452530  [19200/60000]\n",
      "loss: 0.341065  [25600/60000]\n",
      "loss: 0.404610  [32000/60000]\n",
      "loss: 0.401834  [38400/60000]\n",
      "loss: 0.562945  [44800/60000]\n",
      "loss: 0.506590  [51200/60000]\n",
      "loss: 0.375741  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.438582 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.266505  [    0/60000]\n",
      "loss: 0.410700  [ 6400/60000]\n",
      "loss: 0.252957  [12800/60000]\n",
      "loss: 0.449308  [19200/60000]\n",
      "loss: 0.337457  [25600/60000]\n",
      "loss: 0.402620  [32000/60000]\n",
      "loss: 0.398538  [38400/60000]\n",
      "loss: 0.559340  [44800/60000]\n",
      "loss: 0.503199  [51200/60000]\n",
      "loss: 0.374893  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.436608 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.264567  [    0/60000]\n",
      "loss: 0.407883  [ 6400/60000]\n",
      "loss: 0.251372  [12800/60000]\n",
      "loss: 0.446096  [19200/60000]\n",
      "loss: 0.334094  [25600/60000]\n",
      "loss: 0.400657  [32000/60000]\n",
      "loss: 0.395469  [38400/60000]\n",
      "loss: 0.556044  [44800/60000]\n",
      "loss: 0.500206  [51200/60000]\n",
      "loss: 0.374019  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.434713 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.262782  [    0/60000]\n",
      "loss: 0.405201  [ 6400/60000]\n",
      "loss: 0.249883  [12800/60000]\n",
      "loss: 0.443067  [19200/60000]\n",
      "loss: 0.331088  [25600/60000]\n",
      "loss: 0.398694  [32000/60000]\n",
      "loss: 0.392325  [38400/60000]\n",
      "loss: 0.552751  [44800/60000]\n",
      "loss: 0.497239  [51200/60000]\n",
      "loss: 0.373269  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.432856 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.261067  [    0/60000]\n",
      "loss: 0.402594  [ 6400/60000]\n",
      "loss: 0.248438  [12800/60000]\n",
      "loss: 0.440008  [19200/60000]\n",
      "loss: 0.328187  [25600/60000]\n",
      "loss: 0.396738  [32000/60000]\n",
      "loss: 0.389185  [38400/60000]\n",
      "loss: 0.549631  [44800/60000]\n",
      "loss: 0.494201  [51200/60000]\n",
      "loss: 0.372491  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.431064 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.259525  [    0/60000]\n",
      "loss: 0.400239  [ 6400/60000]\n",
      "loss: 0.247102  [12800/60000]\n",
      "loss: 0.437122  [19200/60000]\n",
      "loss: 0.325310  [25600/60000]\n",
      "loss: 0.394773  [32000/60000]\n",
      "loss: 0.386387  [38400/60000]\n",
      "loss: 0.546883  [44800/60000]\n",
      "loss: 0.491166  [51200/60000]\n",
      "loss: 0.371949  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.429322 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.257977  [    0/60000]\n",
      "loss: 0.398111  [ 6400/60000]\n",
      "loss: 0.245774  [12800/60000]\n",
      "loss: 0.434373  [19200/60000]\n",
      "loss: 0.322447  [25600/60000]\n",
      "loss: 0.392742  [32000/60000]\n",
      "loss: 0.383592  [38400/60000]\n",
      "loss: 0.543998  [44800/60000]\n",
      "loss: 0.488463  [51200/60000]\n",
      "loss: 0.371167  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.427647 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.256603  [    0/60000]\n",
      "loss: 0.395955  [ 6400/60000]\n",
      "loss: 0.244432  [12800/60000]\n",
      "loss: 0.431661  [19200/60000]\n",
      "loss: 0.319569  [25600/60000]\n",
      "loss: 0.390807  [32000/60000]\n",
      "loss: 0.380902  [38400/60000]\n",
      "loss: 0.541412  [44800/60000]\n",
      "loss: 0.485773  [51200/60000]\n",
      "loss: 0.370553  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.426062 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.255203  [    0/60000]\n",
      "loss: 0.393842  [ 6400/60000]\n",
      "loss: 0.243164  [12800/60000]\n",
      "loss: 0.428841  [19200/60000]\n",
      "loss: 0.316767  [25600/60000]\n",
      "loss: 0.388789  [32000/60000]\n",
      "loss: 0.378311  [38400/60000]\n",
      "loss: 0.538951  [44800/60000]\n",
      "loss: 0.483427  [51200/60000]\n",
      "loss: 0.369869  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.424477 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.253882  [    0/60000]\n",
      "loss: 0.391908  [ 6400/60000]\n",
      "loss: 0.242041  [12800/60000]\n",
      "loss: 0.426154  [19200/60000]\n",
      "loss: 0.314142  [25600/60000]\n",
      "loss: 0.386997  [32000/60000]\n",
      "loss: 0.375622  [38400/60000]\n",
      "loss: 0.536520  [44800/60000]\n",
      "loss: 0.481172  [51200/60000]\n",
      "loss: 0.369005  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.422986 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.252532  [    0/60000]\n",
      "loss: 0.389966  [ 6400/60000]\n",
      "loss: 0.240896  [12800/60000]\n",
      "loss: 0.423530  [19200/60000]\n",
      "loss: 0.311631  [25600/60000]\n",
      "loss: 0.384753  [32000/60000]\n",
      "loss: 0.373162  [38400/60000]\n",
      "loss: 0.534159  [44800/60000]\n",
      "loss: 0.478917  [51200/60000]\n",
      "loss: 0.368342  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.421436 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.251195  [    0/60000]\n",
      "loss: 0.388093  [ 6400/60000]\n",
      "loss: 0.239989  [12800/60000]\n",
      "loss: 0.421005  [19200/60000]\n",
      "loss: 0.309362  [25600/60000]\n",
      "loss: 0.382717  [32000/60000]\n",
      "loss: 0.371017  [38400/60000]\n",
      "loss: 0.532143  [44800/60000]\n",
      "loss: 0.476577  [51200/60000]\n",
      "loss: 0.367943  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.419966 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.250113  [    0/60000]\n",
      "loss: 0.386394  [ 6400/60000]\n",
      "loss: 0.239048  [12800/60000]\n",
      "loss: 0.418439  [19200/60000]\n",
      "loss: 0.307222  [25600/60000]\n",
      "loss: 0.380497  [32000/60000]\n",
      "loss: 0.368373  [38400/60000]\n",
      "loss: 0.529924  [44800/60000]\n",
      "loss: 0.474586  [51200/60000]\n",
      "loss: 0.367331  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.418537 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.248949  [    0/60000]\n",
      "loss: 0.384831  [ 6400/60000]\n",
      "loss: 0.238147  [12800/60000]\n",
      "loss: 0.415996  [19200/60000]\n",
      "loss: 0.305174  [25600/60000]\n",
      "loss: 0.378669  [32000/60000]\n",
      "loss: 0.365960  [38400/60000]\n",
      "loss: 0.527961  [44800/60000]\n",
      "loss: 0.472563  [51200/60000]\n",
      "loss: 0.366673  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.417126 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.247859  [    0/60000]\n",
      "loss: 0.383068  [ 6400/60000]\n",
      "loss: 0.237215  [12800/60000]\n",
      "loss: 0.413385  [19200/60000]\n",
      "loss: 0.303023  [25600/60000]\n",
      "loss: 0.376653  [32000/60000]\n",
      "loss: 0.363685  [38400/60000]\n",
      "loss: 0.525840  [44800/60000]\n",
      "loss: 0.470580  [51200/60000]\n",
      "loss: 0.366038  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.2%, Avg loss: 0.415745 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.246871  [    0/60000]\n",
      "loss: 0.381808  [ 6400/60000]\n",
      "loss: 0.236309  [12800/60000]\n",
      "loss: 0.410837  [19200/60000]\n",
      "loss: 0.301187  [25600/60000]\n",
      "loss: 0.374844  [32000/60000]\n",
      "loss: 0.361316  [38400/60000]\n",
      "loss: 0.523480  [44800/60000]\n",
      "loss: 0.468780  [51200/60000]\n",
      "loss: 0.365296  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.414462 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.245874  [    0/60000]\n",
      "loss: 0.380297  [ 6400/60000]\n",
      "loss: 0.235580  [12800/60000]\n",
      "loss: 0.408345  [19200/60000]\n",
      "loss: 0.299320  [25600/60000]\n",
      "loss: 0.372975  [32000/60000]\n",
      "loss: 0.358963  [38400/60000]\n",
      "loss: 0.521576  [44800/60000]\n",
      "loss: 0.466916  [51200/60000]\n",
      "loss: 0.364866  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.413165 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.244763  [    0/60000]\n",
      "loss: 0.378770  [ 6400/60000]\n",
      "loss: 0.234837  [12800/60000]\n",
      "loss: 0.405834  [19200/60000]\n",
      "loss: 0.297558  [25600/60000]\n",
      "loss: 0.371274  [32000/60000]\n",
      "loss: 0.356744  [38400/60000]\n",
      "loss: 0.519303  [44800/60000]\n",
      "loss: 0.465141  [51200/60000]\n",
      "loss: 0.364297  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.411899 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.243880  [    0/60000]\n",
      "loss: 0.377333  [ 6400/60000]\n",
      "loss: 0.234096  [12800/60000]\n",
      "loss: 0.403305  [19200/60000]\n",
      "loss: 0.295863  [25600/60000]\n",
      "loss: 0.369533  [32000/60000]\n",
      "loss: 0.354718  [38400/60000]\n",
      "loss: 0.517251  [44800/60000]\n",
      "loss: 0.463088  [51200/60000]\n",
      "loss: 0.363960  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.410649 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.242952  [    0/60000]\n",
      "loss: 0.375961  [ 6400/60000]\n",
      "loss: 0.233365  [12800/60000]\n",
      "loss: 0.401000  [19200/60000]\n",
      "loss: 0.294239  [25600/60000]\n",
      "loss: 0.367700  [32000/60000]\n",
      "loss: 0.352927  [38400/60000]\n",
      "loss: 0.515203  [44800/60000]\n",
      "loss: 0.460941  [51200/60000]\n",
      "loss: 0.363247  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.409452 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.242234  [    0/60000]\n",
      "loss: 0.374529  [ 6400/60000]\n",
      "loss: 0.232644  [12800/60000]\n",
      "loss: 0.398809  [19200/60000]\n",
      "loss: 0.292806  [25600/60000]\n",
      "loss: 0.365862  [32000/60000]\n",
      "loss: 0.351221  [38400/60000]\n",
      "loss: 0.513329  [44800/60000]\n",
      "loss: 0.458901  [51200/60000]\n",
      "loss: 0.362670  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.408280 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.241494  [    0/60000]\n",
      "loss: 0.373475  [ 6400/60000]\n",
      "loss: 0.231894  [12800/60000]\n",
      "loss: 0.396555  [19200/60000]\n",
      "loss: 0.291244  [25600/60000]\n",
      "loss: 0.363808  [32000/60000]\n",
      "loss: 0.349443  [38400/60000]\n",
      "loss: 0.511457  [44800/60000]\n",
      "loss: 0.456827  [51200/60000]\n",
      "loss: 0.361972  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.407037 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.240436  [    0/60000]\n",
      "loss: 0.372048  [ 6400/60000]\n",
      "loss: 0.231306  [12800/60000]\n",
      "loss: 0.394138  [19200/60000]\n",
      "loss: 0.289639  [25600/60000]\n",
      "loss: 0.361857  [32000/60000]\n",
      "loss: 0.347833  [38400/60000]\n",
      "loss: 0.509382  [44800/60000]\n",
      "loss: 0.454993  [51200/60000]\n",
      "loss: 0.361610  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.405892 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.239622  [    0/60000]\n",
      "loss: 0.370886  [ 6400/60000]\n",
      "loss: 0.230480  [12800/60000]\n",
      "loss: 0.391729  [19200/60000]\n",
      "loss: 0.288337  [25600/60000]\n",
      "loss: 0.360009  [32000/60000]\n",
      "loss: 0.346069  [38400/60000]\n",
      "loss: 0.507048  [44800/60000]\n",
      "loss: 0.453213  [51200/60000]\n",
      "loss: 0.361267  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.404775 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.238814  [    0/60000]\n",
      "loss: 0.369685  [ 6400/60000]\n",
      "loss: 0.229781  [12800/60000]\n",
      "loss: 0.389707  [19200/60000]\n",
      "loss: 0.286791  [25600/60000]\n",
      "loss: 0.358448  [32000/60000]\n",
      "loss: 0.344400  [38400/60000]\n",
      "loss: 0.504805  [44800/60000]\n",
      "loss: 0.451225  [51200/60000]\n",
      "loss: 0.360932  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.403636 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.238054  [    0/60000]\n",
      "loss: 0.368582  [ 6400/60000]\n",
      "loss: 0.228948  [12800/60000]\n",
      "loss: 0.387709  [19200/60000]\n",
      "loss: 0.285286  [25600/60000]\n",
      "loss: 0.356551  [32000/60000]\n",
      "loss: 0.342850  [38400/60000]\n",
      "loss: 0.503035  [44800/60000]\n",
      "loss: 0.449033  [51200/60000]\n",
      "loss: 0.360378  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.402539 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.237317  [    0/60000]\n",
      "loss: 0.367326  [ 6400/60000]\n",
      "loss: 0.228006  [12800/60000]\n",
      "loss: 0.385301  [19200/60000]\n",
      "loss: 0.284125  [25600/60000]\n",
      "loss: 0.355044  [32000/60000]\n",
      "loss: 0.341196  [38400/60000]\n",
      "loss: 0.500816  [44800/60000]\n",
      "loss: 0.447297  [51200/60000]\n",
      "loss: 0.360261  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.401492 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.236734  [    0/60000]\n",
      "loss: 0.366119  [ 6400/60000]\n",
      "loss: 0.226967  [12800/60000]\n",
      "loss: 0.383294  [19200/60000]\n",
      "loss: 0.282774  [25600/60000]\n",
      "loss: 0.353526  [32000/60000]\n",
      "loss: 0.339797  [38400/60000]\n",
      "loss: 0.498765  [44800/60000]\n",
      "loss: 0.445655  [51200/60000]\n",
      "loss: 0.359800  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.400477 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.235998  [    0/60000]\n",
      "loss: 0.364974  [ 6400/60000]\n",
      "loss: 0.226141  [12800/60000]\n",
      "loss: 0.381142  [19200/60000]\n",
      "loss: 0.281730  [25600/60000]\n",
      "loss: 0.351822  [32000/60000]\n",
      "loss: 0.338468  [38400/60000]\n",
      "loss: 0.496722  [44800/60000]\n",
      "loss: 0.444039  [51200/60000]\n",
      "loss: 0.359444  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.399423 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.235369  [    0/60000]\n",
      "loss: 0.363775  [ 6400/60000]\n",
      "loss: 0.225388  [12800/60000]\n",
      "loss: 0.379031  [19200/60000]\n",
      "loss: 0.280657  [25600/60000]\n",
      "loss: 0.350298  [32000/60000]\n",
      "loss: 0.336848  [38400/60000]\n",
      "loss: 0.494970  [44800/60000]\n",
      "loss: 0.442142  [51200/60000]\n",
      "loss: 0.358965  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.398413 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.234655  [    0/60000]\n",
      "loss: 0.362397  [ 6400/60000]\n",
      "loss: 0.224673  [12800/60000]\n",
      "loss: 0.377314  [19200/60000]\n",
      "loss: 0.279444  [25600/60000]\n",
      "loss: 0.348730  [32000/60000]\n",
      "loss: 0.335558  [38400/60000]\n",
      "loss: 0.493296  [44800/60000]\n",
      "loss: 0.439850  [51200/60000]\n",
      "loss: 0.358419  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.397300 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.234017  [    0/60000]\n",
      "loss: 0.361198  [ 6400/60000]\n",
      "loss: 0.223842  [12800/60000]\n",
      "loss: 0.375203  [19200/60000]\n",
      "loss: 0.278400  [25600/60000]\n",
      "loss: 0.347328  [32000/60000]\n",
      "loss: 0.334173  [38400/60000]\n",
      "loss: 0.491472  [44800/60000]\n",
      "loss: 0.438095  [51200/60000]\n",
      "loss: 0.358443  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.396385 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.233348  [    0/60000]\n",
      "loss: 0.359791  [ 6400/60000]\n",
      "loss: 0.223187  [12800/60000]\n",
      "loss: 0.373115  [19200/60000]\n",
      "loss: 0.277199  [25600/60000]\n",
      "loss: 0.345634  [32000/60000]\n",
      "loss: 0.333510  [38400/60000]\n",
      "loss: 0.489851  [44800/60000]\n",
      "loss: 0.436435  [51200/60000]\n",
      "loss: 0.357951  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.395410 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.232880  [    0/60000]\n",
      "loss: 0.358488  [ 6400/60000]\n",
      "loss: 0.222398  [12800/60000]\n",
      "loss: 0.370993  [19200/60000]\n",
      "loss: 0.276137  [25600/60000]\n",
      "loss: 0.344429  [32000/60000]\n",
      "loss: 0.332521  [38400/60000]\n",
      "loss: 0.488142  [44800/60000]\n",
      "loss: 0.434448  [51200/60000]\n",
      "loss: 0.357261  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.394420 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.232277  [    0/60000]\n",
      "loss: 0.357149  [ 6400/60000]\n",
      "loss: 0.221413  [12800/60000]\n",
      "loss: 0.368932  [19200/60000]\n",
      "loss: 0.275015  [25600/60000]\n",
      "loss: 0.343070  [32000/60000]\n",
      "loss: 0.331248  [38400/60000]\n",
      "loss: 0.486481  [44800/60000]\n",
      "loss: 0.432796  [51200/60000]\n",
      "loss: 0.356788  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.393418 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.231557  [    0/60000]\n",
      "loss: 0.355695  [ 6400/60000]\n",
      "loss: 0.220298  [12800/60000]\n",
      "loss: 0.366998  [19200/60000]\n",
      "loss: 0.273861  [25600/60000]\n",
      "loss: 0.341691  [32000/60000]\n",
      "loss: 0.330296  [38400/60000]\n",
      "loss: 0.484855  [44800/60000]\n",
      "loss: 0.430920  [51200/60000]\n",
      "loss: 0.356828  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.392467 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.230924  [    0/60000]\n",
      "loss: 0.354180  [ 6400/60000]\n",
      "loss: 0.219427  [12800/60000]\n",
      "loss: 0.365037  [19200/60000]\n",
      "loss: 0.272708  [25600/60000]\n",
      "loss: 0.340437  [32000/60000]\n",
      "loss: 0.329669  [38400/60000]\n",
      "loss: 0.483269  [44800/60000]\n",
      "loss: 0.428849  [51200/60000]\n",
      "loss: 0.356405  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.391584 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.230020  [    0/60000]\n",
      "loss: 0.352780  [ 6400/60000]\n",
      "loss: 0.218927  [12800/60000]\n",
      "loss: 0.363319  [19200/60000]\n",
      "loss: 0.271793  [25600/60000]\n",
      "loss: 0.339222  [32000/60000]\n",
      "loss: 0.328631  [38400/60000]\n",
      "loss: 0.481521  [44800/60000]\n",
      "loss: 0.427059  [51200/60000]\n",
      "loss: 0.356127  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.390616 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.229336  [    0/60000]\n",
      "loss: 0.351669  [ 6400/60000]\n",
      "loss: 0.217929  [12800/60000]\n",
      "loss: 0.361515  [19200/60000]\n",
      "loss: 0.270581  [25600/60000]\n",
      "loss: 0.337962  [32000/60000]\n",
      "loss: 0.327585  [38400/60000]\n",
      "loss: 0.479399  [44800/60000]\n",
      "loss: 0.425190  [51200/60000]\n",
      "loss: 0.355599  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.389661 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.228475  [    0/60000]\n",
      "loss: 0.350419  [ 6400/60000]\n",
      "loss: 0.217209  [12800/60000]\n",
      "loss: 0.359783  [19200/60000]\n",
      "loss: 0.269611  [25600/60000]\n",
      "loss: 0.336986  [32000/60000]\n",
      "loss: 0.326794  [38400/60000]\n",
      "loss: 0.477406  [44800/60000]\n",
      "loss: 0.423343  [51200/60000]\n",
      "loss: 0.354528  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.388718 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.227548  [    0/60000]\n",
      "loss: 0.349424  [ 6400/60000]\n",
      "loss: 0.216673  [12800/60000]\n",
      "loss: 0.358116  [19200/60000]\n",
      "loss: 0.268793  [25600/60000]\n",
      "loss: 0.335662  [32000/60000]\n",
      "loss: 0.326040  [38400/60000]\n",
      "loss: 0.475656  [44800/60000]\n",
      "loss: 0.421723  [51200/60000]\n",
      "loss: 0.354187  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.387870 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.226769  [    0/60000]\n",
      "loss: 0.348058  [ 6400/60000]\n",
      "loss: 0.215837  [12800/60000]\n",
      "loss: 0.356318  [19200/60000]\n",
      "loss: 0.268160  [25600/60000]\n",
      "loss: 0.334229  [32000/60000]\n",
      "loss: 0.325154  [38400/60000]\n",
      "loss: 0.474553  [44800/60000]\n",
      "loss: 0.420075  [51200/60000]\n",
      "loss: 0.353847  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.386975 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.226008  [    0/60000]\n",
      "loss: 0.346835  [ 6400/60000]\n",
      "loss: 0.215185  [12800/60000]\n",
      "loss: 0.354597  [19200/60000]\n",
      "loss: 0.267671  [25600/60000]\n",
      "loss: 0.333298  [32000/60000]\n",
      "loss: 0.324233  [38400/60000]\n",
      "loss: 0.472807  [44800/60000]\n",
      "loss: 0.418354  [51200/60000]\n",
      "loss: 0.353136  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.386119 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.225427  [    0/60000]\n",
      "loss: 0.345670  [ 6400/60000]\n",
      "loss: 0.214607  [12800/60000]\n",
      "loss: 0.352662  [19200/60000]\n",
      "loss: 0.267057  [25600/60000]\n",
      "loss: 0.332155  [32000/60000]\n",
      "loss: 0.323695  [38400/60000]\n",
      "loss: 0.471411  [44800/60000]\n",
      "loss: 0.416267  [51200/60000]\n",
      "loss: 0.352694  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.385183 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.224683  [    0/60000]\n",
      "loss: 0.344457  [ 6400/60000]\n",
      "loss: 0.213957  [12800/60000]\n",
      "loss: 0.350842  [19200/60000]\n",
      "loss: 0.266740  [25600/60000]\n",
      "loss: 0.331170  [32000/60000]\n",
      "loss: 0.322515  [38400/60000]\n",
      "loss: 0.470313  [44800/60000]\n",
      "loss: 0.414276  [51200/60000]\n",
      "loss: 0.351960  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.384363 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.223913  [    0/60000]\n",
      "loss: 0.343568  [ 6400/60000]\n",
      "loss: 0.213140  [12800/60000]\n",
      "loss: 0.349041  [19200/60000]\n",
      "loss: 0.266211  [25600/60000]\n",
      "loss: 0.330434  [32000/60000]\n",
      "loss: 0.321421  [38400/60000]\n",
      "loss: 0.468528  [44800/60000]\n",
      "loss: 0.412519  [51200/60000]\n",
      "loss: 0.351822  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.383476 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.223242  [    0/60000]\n",
      "loss: 0.342089  [ 6400/60000]\n",
      "loss: 0.212363  [12800/60000]\n",
      "loss: 0.347649  [19200/60000]\n",
      "loss: 0.265346  [25600/60000]\n",
      "loss: 0.329384  [32000/60000]\n",
      "loss: 0.320325  [38400/60000]\n",
      "loss: 0.466642  [44800/60000]\n",
      "loss: 0.410456  [51200/60000]\n",
      "loss: 0.351178  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.382687 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.222653  [    0/60000]\n",
      "loss: 0.340901  [ 6400/60000]\n",
      "loss: 0.211675  [12800/60000]\n",
      "loss: 0.345744  [19200/60000]\n",
      "loss: 0.264837  [25600/60000]\n",
      "loss: 0.328472  [32000/60000]\n",
      "loss: 0.319403  [38400/60000]\n",
      "loss: 0.465211  [44800/60000]\n",
      "loss: 0.408659  [51200/60000]\n",
      "loss: 0.350680  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.381928 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.221865  [    0/60000]\n",
      "loss: 0.339734  [ 6400/60000]\n",
      "loss: 0.210919  [12800/60000]\n",
      "loss: 0.344129  [19200/60000]\n",
      "loss: 0.264279  [25600/60000]\n",
      "loss: 0.327733  [32000/60000]\n",
      "loss: 0.318460  [38400/60000]\n",
      "loss: 0.463853  [44800/60000]\n",
      "loss: 0.406781  [51200/60000]\n",
      "loss: 0.350030  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.381150 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.221197  [    0/60000]\n",
      "loss: 0.338991  [ 6400/60000]\n",
      "loss: 0.209970  [12800/60000]\n",
      "loss: 0.342487  [19200/60000]\n",
      "loss: 0.263622  [25600/60000]\n",
      "loss: 0.326730  [32000/60000]\n",
      "loss: 0.317370  [38400/60000]\n",
      "loss: 0.462236  [44800/60000]\n",
      "loss: 0.404707  [51200/60000]\n",
      "loss: 0.349752  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.380415 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.220459  [    0/60000]\n",
      "loss: 0.337757  [ 6400/60000]\n",
      "loss: 0.209059  [12800/60000]\n",
      "loss: 0.341202  [19200/60000]\n",
      "loss: 0.262945  [25600/60000]\n",
      "loss: 0.325780  [32000/60000]\n",
      "loss: 0.316536  [38400/60000]\n",
      "loss: 0.460411  [44800/60000]\n",
      "loss: 0.402814  [51200/60000]\n",
      "loss: 0.349342  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.379649 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.219893  [    0/60000]\n",
      "loss: 0.336467  [ 6400/60000]\n",
      "loss: 0.208296  [12800/60000]\n",
      "loss: 0.339559  [19200/60000]\n",
      "loss: 0.262508  [25600/60000]\n",
      "loss: 0.324981  [32000/60000]\n",
      "loss: 0.315583  [38400/60000]\n",
      "loss: 0.458295  [44800/60000]\n",
      "loss: 0.400793  [51200/60000]\n",
      "loss: 0.348929  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.378932 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.219566  [    0/60000]\n",
      "loss: 0.335233  [ 6400/60000]\n",
      "loss: 0.207541  [12800/60000]\n",
      "loss: 0.337866  [19200/60000]\n",
      "loss: 0.262157  [25600/60000]\n",
      "loss: 0.324232  [32000/60000]\n",
      "loss: 0.314473  [38400/60000]\n",
      "loss: 0.456868  [44800/60000]\n",
      "loss: 0.399165  [51200/60000]\n",
      "loss: 0.348480  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.378189 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.218937  [    0/60000]\n",
      "loss: 0.334176  [ 6400/60000]\n",
      "loss: 0.207010  [12800/60000]\n",
      "loss: 0.336194  [19200/60000]\n",
      "loss: 0.261456  [25600/60000]\n",
      "loss: 0.323334  [32000/60000]\n",
      "loss: 0.313904  [38400/60000]\n",
      "loss: 0.455459  [44800/60000]\n",
      "loss: 0.397412  [51200/60000]\n",
      "loss: 0.347997  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.377474 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.218509  [    0/60000]\n",
      "loss: 0.333043  [ 6400/60000]\n",
      "loss: 0.206031  [12800/60000]\n",
      "loss: 0.334255  [19200/60000]\n",
      "loss: 0.260885  [25600/60000]\n",
      "loss: 0.322664  [32000/60000]\n",
      "loss: 0.312755  [38400/60000]\n",
      "loss: 0.453616  [44800/60000]\n",
      "loss: 0.395385  [51200/60000]\n",
      "loss: 0.347451  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.376700 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.217824  [    0/60000]\n",
      "loss: 0.332049  [ 6400/60000]\n",
      "loss: 0.205397  [12800/60000]\n",
      "loss: 0.332760  [19200/60000]\n",
      "loss: 0.260489  [25600/60000]\n",
      "loss: 0.321998  [32000/60000]\n",
      "loss: 0.312259  [38400/60000]\n",
      "loss: 0.451631  [44800/60000]\n",
      "loss: 0.393261  [51200/60000]\n",
      "loss: 0.347220  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.375947 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.217009  [    0/60000]\n",
      "loss: 0.330582  [ 6400/60000]\n",
      "loss: 0.204767  [12800/60000]\n",
      "loss: 0.331409  [19200/60000]\n",
      "loss: 0.260025  [25600/60000]\n",
      "loss: 0.321473  [32000/60000]\n",
      "loss: 0.311637  [38400/60000]\n",
      "loss: 0.449914  [44800/60000]\n",
      "loss: 0.391387  [51200/60000]\n",
      "loss: 0.346713  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.375284 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.216385  [    0/60000]\n",
      "loss: 0.329636  [ 6400/60000]\n",
      "loss: 0.203888  [12800/60000]\n",
      "loss: 0.329692  [19200/60000]\n",
      "loss: 0.259795  [25600/60000]\n",
      "loss: 0.320581  [32000/60000]\n",
      "loss: 0.310664  [38400/60000]\n",
      "loss: 0.447975  [44800/60000]\n",
      "loss: 0.389802  [51200/60000]\n",
      "loss: 0.346585  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.374611 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.216198  [    0/60000]\n",
      "loss: 0.328283  [ 6400/60000]\n",
      "loss: 0.203228  [12800/60000]\n",
      "loss: 0.327991  [19200/60000]\n",
      "loss: 0.259668  [25600/60000]\n",
      "loss: 0.320098  [32000/60000]\n",
      "loss: 0.309788  [38400/60000]\n",
      "loss: 0.446051  [44800/60000]\n",
      "loss: 0.387874  [51200/60000]\n",
      "loss: 0.346004  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.373946 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.215460  [    0/60000]\n",
      "loss: 0.327051  [ 6400/60000]\n",
      "loss: 0.202706  [12800/60000]\n",
      "loss: 0.326380  [19200/60000]\n",
      "loss: 0.259349  [25600/60000]\n",
      "loss: 0.319310  [32000/60000]\n",
      "loss: 0.309260  [38400/60000]\n",
      "loss: 0.443715  [44800/60000]\n",
      "loss: 0.386174  [51200/60000]\n",
      "loss: 0.345663  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.373226 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.215030  [    0/60000]\n",
      "loss: 0.325655  [ 6400/60000]\n",
      "loss: 0.202205  [12800/60000]\n",
      "loss: 0.325214  [19200/60000]\n",
      "loss: 0.258926  [25600/60000]\n",
      "loss: 0.319011  [32000/60000]\n",
      "loss: 0.308645  [38400/60000]\n",
      "loss: 0.441609  [44800/60000]\n",
      "loss: 0.384086  [51200/60000]\n",
      "loss: 0.345369  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.372601 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.214519  [    0/60000]\n",
      "loss: 0.324294  [ 6400/60000]\n",
      "loss: 0.201632  [12800/60000]\n",
      "loss: 0.323871  [19200/60000]\n",
      "loss: 0.258329  [25600/60000]\n",
      "loss: 0.318612  [32000/60000]\n",
      "loss: 0.307898  [38400/60000]\n",
      "loss: 0.439482  [44800/60000]\n",
      "loss: 0.382626  [51200/60000]\n",
      "loss: 0.344546  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.371990 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.213624  [    0/60000]\n",
      "loss: 0.323201  [ 6400/60000]\n",
      "loss: 0.201127  [12800/60000]\n",
      "loss: 0.322120  [19200/60000]\n",
      "loss: 0.258269  [25600/60000]\n",
      "loss: 0.318001  [32000/60000]\n",
      "loss: 0.306978  [38400/60000]\n",
      "loss: 0.437791  [44800/60000]\n",
      "loss: 0.380509  [51200/60000]\n",
      "loss: 0.344404  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.371289 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.213308  [    0/60000]\n",
      "loss: 0.322098  [ 6400/60000]\n",
      "loss: 0.200547  [12800/60000]\n",
      "loss: 0.320640  [19200/60000]\n",
      "loss: 0.257984  [25600/60000]\n",
      "loss: 0.317430  [32000/60000]\n",
      "loss: 0.306124  [38400/60000]\n",
      "loss: 0.435937  [44800/60000]\n",
      "loss: 0.378769  [51200/60000]\n",
      "loss: 0.343899  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.370620 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.213221  [    0/60000]\n",
      "loss: 0.320797  [ 6400/60000]\n",
      "loss: 0.199805  [12800/60000]\n",
      "loss: 0.319111  [19200/60000]\n",
      "loss: 0.257668  [25600/60000]\n",
      "loss: 0.316736  [32000/60000]\n",
      "loss: 0.305341  [38400/60000]\n",
      "loss: 0.433950  [44800/60000]\n",
      "loss: 0.376627  [51200/60000]\n",
      "loss: 0.343504  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.370008 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.212336  [    0/60000]\n",
      "loss: 0.319934  [ 6400/60000]\n",
      "loss: 0.199509  [12800/60000]\n",
      "loss: 0.317634  [19200/60000]\n",
      "loss: 0.257600  [25600/60000]\n",
      "loss: 0.316293  [32000/60000]\n",
      "loss: 0.304567  [38400/60000]\n",
      "loss: 0.431979  [44800/60000]\n",
      "loss: 0.374832  [51200/60000]\n",
      "loss: 0.343359  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.369341 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn) # inference 시에 optimizer는 불필요\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b098f22-fdb6-4656-a75d-4184b69f6226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[ 0.0248,  0.0094,  0.0336,  ...,  0.0028,  0.0300,  0.0082],\n",
       "                      [ 0.0122,  0.0120, -0.0343,  ..., -0.0235, -0.0050, -0.0122],\n",
       "                      [ 0.0187, -0.0114,  0.0136,  ...,  0.0224,  0.0055,  0.0029],\n",
       "                      ...,\n",
       "                      [ 0.0122,  0.0339, -0.0101,  ...,  0.0075, -0.0175,  0.0329],\n",
       "                      [ 0.0090, -0.0021,  0.0293,  ...,  0.0200,  0.0103, -0.0305],\n",
       "                      [-0.0166, -0.0238, -0.0070,  ..., -0.0097,  0.0269, -0.0331]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([-0.0218, -0.0413,  0.0179, -0.0145,  0.0029, -0.0590,  0.0236, -0.0268,\n",
       "                      -0.0047,  0.0570, -0.0003, -0.0379,  0.0467, -0.0470,  0.0036,  0.0139,\n",
       "                      -0.0286, -0.0039, -0.0054,  0.0204, -0.0744,  0.0155, -0.0141, -0.0008,\n",
       "                       0.0747, -0.0305,  0.0523,  0.0507, -0.0581, -0.0300,  0.0624,  0.0046,\n",
       "                       0.0530, -0.0104, -0.0125, -0.0471,  0.0097,  0.0604,  0.0769, -0.0492,\n",
       "                       0.0189,  0.0425,  0.0066,  0.0299, -0.0101,  0.0375,  0.0141, -0.0180,\n",
       "                       0.0084,  0.0317,  0.0785,  0.0502, -0.0395, -0.0011,  0.0215, -0.0351,\n",
       "                       0.0454, -0.0203, -0.0037,  0.0165, -0.0415, -0.0070, -0.0130, -0.0485,\n",
       "                      -0.0505,  0.0768,  0.0356, -0.0104,  0.0284,  0.0017,  0.0953, -0.0255,\n",
       "                      -0.0081, -0.0112,  0.0093, -0.0041,  0.0715,  0.0181,  0.0634,  0.0008,\n",
       "                       0.0248,  0.0970, -0.0024, -0.0490,  0.0233, -0.0253,  0.0219, -0.0427,\n",
       "                       0.0010,  0.0268,  0.0514,  0.1101, -0.0082,  0.0374,  0.0009, -0.0335,\n",
       "                      -0.0133,  0.0737,  0.0038, -0.0226, -0.0512, -0.0334,  0.0697,  0.0176,\n",
       "                      -0.0337, -0.0311,  0.0733,  0.0091,  0.0180,  0.0336,  0.0611, -0.0126,\n",
       "                      -0.0432,  0.0241, -0.0049,  0.0042,  0.0438, -0.0203,  0.0344,  0.0075,\n",
       "                      -0.0487, -0.0385, -0.0476,  0.0638,  0.0309,  0.0261,  0.0120, -0.0149,\n",
       "                      -0.0118, -0.0393,  0.0254,  0.0561,  0.0120, -0.0044, -0.0487,  0.0216,\n",
       "                       0.0120,  0.0279,  0.0164,  0.0236, -0.0126, -0.0270, -0.0090, -0.0155,\n",
       "                       0.0343,  0.0317,  0.0554,  0.0174,  0.0193,  0.0027,  0.0096, -0.0243,\n",
       "                       0.0156, -0.0671,  0.0198, -0.0279, -0.0035,  0.0222, -0.0614,  0.0187,\n",
       "                      -0.0582, -0.0275,  0.0102,  0.0346,  0.0160, -0.0380, -0.0492, -0.0123,\n",
       "                       0.0368, -0.0421,  0.0307,  0.0116,  0.0013,  0.0722, -0.0217, -0.0374,\n",
       "                       0.0024,  0.0488,  0.0602, -0.0459,  0.0353,  0.0195, -0.0192, -0.0132,\n",
       "                       0.0226,  0.0196, -0.0149, -0.0663, -0.0051, -0.0441,  0.1010, -0.0547,\n",
       "                       0.0459, -0.0410,  0.0595,  0.0247, -0.0239, -0.0875,  0.0532, -0.0053,\n",
       "                       0.0133,  0.0169,  0.0206, -0.0307,  0.1212,  0.0187,  0.0841,  0.0161,\n",
       "                      -0.0361, -0.0230, -0.0283, -0.0158, -0.0201,  0.0278, -0.0293,  0.0610,\n",
       "                      -0.0037,  0.0087,  0.0450, -0.0797, -0.0143,  0.1422, -0.0246, -0.0143,\n",
       "                      -0.0170,  0.0153,  0.0142,  0.0011,  0.0421, -0.0031, -0.0041, -0.0204,\n",
       "                      -0.0550, -0.0011,  0.0153,  0.0617, -0.0058, -0.0191, -0.0716,  0.0277,\n",
       "                       0.0593, -0.0589,  0.0292, -0.0072,  0.0449,  0.0429,  0.0871,  0.0271,\n",
       "                       0.0333, -0.0025,  0.0204,  0.0628,  0.0617, -0.0265,  0.0539,  0.0245,\n",
       "                      -0.0168, -0.0027,  0.0052, -0.0016,  0.0445, -0.0315, -0.0112, -0.0051,\n",
       "                      -0.0126,  0.0478,  0.0835,  0.0043, -0.0774,  0.0973, -0.0108, -0.0378,\n",
       "                       0.0391,  0.0354, -0.0065, -0.0096,  0.0498, -0.0305,  0.0141, -0.0321,\n",
       "                       0.0275, -0.0257, -0.0238,  0.0419, -0.0402,  0.0316,  0.0100,  0.0589,\n",
       "                      -0.0163, -0.0168,  0.0063,  0.0096,  0.0662,  0.0109,  0.0358,  0.0346,\n",
       "                       0.0379, -0.0334, -0.0345,  0.0604,  0.0349,  0.1172,  0.0361, -0.0212,\n",
       "                      -0.0250, -0.0005,  0.0660,  0.0353,  0.0118, -0.0014,  0.0088,  0.0167,\n",
       "                      -0.0802,  0.0440, -0.0062,  0.0260, -0.0077,  0.0389,  0.0222,  0.0721,\n",
       "                      -0.0860,  0.0138, -0.0218,  0.0094, -0.0103, -0.0324,  0.0067, -0.0007,\n",
       "                      -0.0102,  0.0019,  0.0219,  0.0168, -0.0352,  0.0378, -0.0125,  0.0039,\n",
       "                       0.0256, -0.0201,  0.0222,  0.0056,  0.0330,  0.0216, -0.0059, -0.0059,\n",
       "                       0.0528,  0.0131, -0.0058, -0.0165, -0.0314,  0.0080, -0.0036, -0.0275,\n",
       "                       0.0720,  0.0537,  0.0383, -0.0303,  0.0431, -0.0111, -0.0291,  0.0100,\n",
       "                      -0.0511, -0.0204,  0.0095, -0.0572, -0.0193,  0.0149, -0.0203, -0.0312,\n",
       "                       0.0838,  0.0190, -0.0564, -0.0042, -0.0196, -0.0122, -0.0060,  0.0391,\n",
       "                       0.0660, -0.0282, -0.0026, -0.0306,  0.0344,  0.0044, -0.0230,  0.0047,\n",
       "                       0.0454,  0.0503,  0.0928,  0.0265,  0.0286,  0.0103, -0.0158,  0.0305,\n",
       "                       0.0189, -0.0416, -0.0119,  0.0723,  0.0152,  0.0871,  0.0282,  0.0305,\n",
       "                       0.0091, -0.0321,  0.0323,  0.0021,  0.0645,  0.0265, -0.0242,  0.0100,\n",
       "                       0.0042,  0.0100,  0.0340, -0.0317,  0.0590,  0.0162,  0.0285,  0.0315,\n",
       "                       0.0070,  0.0151,  0.0048, -0.0589, -0.0264, -0.0354, -0.0068, -0.0089,\n",
       "                       0.0021,  0.0179,  0.1890,  0.0612,  0.0433,  0.0112,  0.0679, -0.0028,\n",
       "                       0.0069, -0.0247,  0.0564,  0.0324,  0.0050, -0.0727,  0.0173,  0.0304,\n",
       "                       0.0989,  0.0239,  0.0487,  0.0518,  0.0318,  0.0691,  0.0155, -0.0263,\n",
       "                      -0.0548,  0.0214,  0.0011, -0.0050,  0.0177, -0.0066,  0.0116,  0.0439,\n",
       "                       0.0350,  0.0346,  0.0034,  0.0178,  0.0240,  0.0311,  0.0364,  0.0601,\n",
       "                      -0.0181, -0.0123,  0.0357,  0.0071, -0.0304, -0.0041,  0.0436, -0.0082,\n",
       "                       0.0213, -0.0047,  0.0519,  0.0828, -0.0185, -0.0013,  0.0222,  0.0040,\n",
       "                       0.0812,  0.0373,  0.1049, -0.0186,  0.0408,  0.0174,  0.0351,  0.0168,\n",
       "                       0.0970,  0.0102,  0.0068,  0.0067, -0.0090,  0.0036,  0.0602,  0.0165,\n",
       "                       0.0069, -0.0823, -0.0438,  0.0439, -0.0613,  0.0133,  0.0435, -0.0035,\n",
       "                      -0.0437, -0.0438, -0.0040,  0.0142,  0.0381, -0.0107,  0.1104, -0.0169],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[-0.0092, -0.0168, -0.0446,  ..., -0.0224,  0.0158, -0.0344],\n",
       "                      [ 0.0282,  0.0222,  0.0376,  ...,  0.0468, -0.0110,  0.0087],\n",
       "                      [-0.0057, -0.0240, -0.0084,  ..., -0.0187, -0.0126, -0.0373],\n",
       "                      ...,\n",
       "                      [ 0.0275,  0.0059,  0.0220,  ...,  0.0321, -0.0143, -0.0341],\n",
       "                      [-0.0167,  0.0395, -0.0039,  ..., -0.0222,  0.0096,  0.0240],\n",
       "                      [-0.0122, -0.0068, -0.0220,  ...,  0.0400,  0.0132, -0.0140]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 1.7197e-02,  1.3090e-02,  1.8396e-02, -3.1755e-02,  3.7074e-02,\n",
       "                       3.5085e-02,  7.2143e-03, -9.2809e-03,  2.7254e-02, -6.2201e-02,\n",
       "                      -3.1176e-02,  1.1212e-01,  1.4369e-02,  1.3831e-03,  4.6727e-02,\n",
       "                      -2.3186e-02, -1.3226e-02, -3.2537e-02, -2.8025e-02, -5.0627e-02,\n",
       "                      -2.0272e-02, -3.5661e-04,  6.9397e-02,  4.3277e-02, -7.5557e-03,\n",
       "                       6.6789e-02,  2.9530e-03,  4.5841e-02,  8.4815e-02,  1.6185e-02,\n",
       "                       6.2545e-02,  8.5582e-02, -2.9928e-02, -9.6313e-03,  4.4886e-02,\n",
       "                       5.0563e-03,  2.0522e-02, -1.4277e-02,  7.0884e-02,  3.3703e-02,\n",
       "                      -5.8714e-02, -1.3823e-02,  1.3410e-02,  5.6335e-02,  1.2595e-02,\n",
       "                      -6.3207e-02,  2.1566e-02, -1.2481e-02,  2.2583e-02,  3.2730e-02,\n",
       "                      -3.8138e-02, -1.6887e-02, -4.8738e-03, -3.8917e-02, -3.2594e-04,\n",
       "                      -2.7059e-02,  1.9180e-02, -1.1764e-02, -7.3661e-03, -1.0644e-03,\n",
       "                       3.3891e-02, -2.2366e-02,  7.6232e-02,  6.5542e-03,  6.1331e-02,\n",
       "                      -2.3510e-02, -2.9535e-02, -2.2934e-02, -1.5403e-02, -5.2325e-02,\n",
       "                      -3.5495e-02,  6.2507e-02,  2.4613e-02, -5.8869e-02,  3.2168e-02,\n",
       "                      -2.3465e-02, -1.4833e-02, -2.2537e-02, -1.5492e-02,  4.8327e-03,\n",
       "                      -2.7456e-02, -1.0292e-02,  1.5239e-02, -3.7999e-02, -6.9450e-02,\n",
       "                       1.1883e-03,  5.0465e-02,  3.3853e-02,  2.2065e-02,  2.5472e-02,\n",
       "                      -1.4820e-02, -3.2686e-02, -2.2274e-02, -2.8259e-02,  1.7459e-02,\n",
       "                      -1.6207e-02,  2.1343e-02,  5.0038e-02,  8.3409e-03, -5.6461e-02,\n",
       "                       5.4820e-02,  2.1602e-02, -7.3863e-02,  9.2844e-03, -9.4058e-03,\n",
       "                       2.0284e-02,  6.3529e-02,  5.9991e-02, -3.9332e-02, -4.2670e-02,\n",
       "                       6.0108e-02, -4.4224e-02,  3.2787e-02,  9.8395e-02, -3.6447e-02,\n",
       "                      -4.6427e-02,  3.3814e-02,  3.9590e-02,  1.2627e-02,  5.4366e-02,\n",
       "                      -1.2466e-03,  1.2628e-02, -1.3761e-02,  1.0366e-02, -4.0031e-02,\n",
       "                      -2.6030e-02, -6.8040e-03,  3.1081e-02,  8.1991e-02, -3.9691e-03,\n",
       "                      -3.3131e-02,  1.3912e-02,  5.8957e-02,  3.4540e-02,  9.4853e-03,\n",
       "                       2.7091e-02,  6.8854e-02,  5.1163e-02, -2.4167e-02, -4.1076e-02,\n",
       "                      -4.7096e-02, -1.4869e-02, -2.2259e-02, -3.3953e-02, -3.3530e-03,\n",
       "                      -1.8593e-02, -7.3075e-02, -3.6462e-02, -4.1606e-03,  1.3571e-02,\n",
       "                       3.7988e-02,  1.0794e-02, -3.4555e-02, -1.1865e-02,  3.0460e-03,\n",
       "                      -1.2398e-02,  5.4771e-02, -3.7742e-02, -4.5723e-02, -5.4961e-02,\n",
       "                       1.2672e-04,  2.5089e-02, -2.5569e-02, -5.6054e-02,  1.8308e-02,\n",
       "                      -5.0760e-03,  5.2696e-02,  1.8899e-02,  7.2599e-02, -2.2239e-02,\n",
       "                       1.0125e-01,  4.6824e-02,  5.9970e-02, -5.5446e-02,  2.6927e-02,\n",
       "                       3.8554e-02,  1.5961e-02,  2.7228e-02,  3.3473e-02, -8.3164e-03,\n",
       "                      -3.0983e-02, -9.8837e-03,  9.8580e-03,  2.5820e-02,  2.5954e-02,\n",
       "                       3.9615e-02,  1.6141e-02,  6.1651e-02,  5.5977e-02, -1.5169e-03,\n",
       "                       6.0744e-02,  2.0911e-02, -8.8376e-03,  9.8810e-02,  6.7779e-02,\n",
       "                      -1.8619e-02,  6.2120e-02,  5.9027e-02,  2.3781e-02,  1.6621e-02,\n",
       "                       2.0120e-03,  4.2047e-02, -2.8394e-02, -1.8452e-02,  2.0655e-02,\n",
       "                       6.1646e-02,  5.2815e-02,  8.1064e-02,  5.7641e-02, -2.6901e-02,\n",
       "                       8.9621e-02,  3.1211e-02,  8.0036e-02,  6.5657e-03, -6.3241e-02,\n",
       "                       6.1988e-02, -2.7469e-02,  9.3174e-02,  4.8080e-02, -4.1513e-02,\n",
       "                       1.1013e-03, -1.4252e-02,  1.2116e-02,  7.6025e-03,  3.1962e-02,\n",
       "                      -2.5482e-02, -3.0716e-03, -6.6909e-02, -4.4987e-02, -7.0898e-02,\n",
       "                       2.1160e-02,  6.2780e-03, -7.1180e-03,  1.9893e-02, -9.8720e-03,\n",
       "                       3.5967e-02,  5.3940e-03, -4.3875e-02,  5.6229e-02, -1.9904e-02,\n",
       "                      -1.3131e-02,  4.7153e-02, -1.0630e-02,  2.7276e-02,  5.6169e-02,\n",
       "                       2.3594e-03, -4.8122e-02,  1.5044e-02,  4.2471e-02,  3.0100e-02,\n",
       "                       4.3049e-02, -3.3593e-02,  2.9392e-02, -3.0062e-03,  1.3148e-02,\n",
       "                       4.6985e-02,  5.1185e-02,  1.2021e-02,  4.4260e-02, -4.9935e-02,\n",
       "                       1.0282e-01, -5.4794e-02, -2.9662e-02,  2.2477e-02,  5.6632e-02,\n",
       "                       5.7558e-04,  2.7457e-02,  5.9786e-02,  8.2236e-04, -1.5510e-02,\n",
       "                      -7.1812e-02,  1.5810e-02,  8.2665e-02, -1.4492e-02, -4.1569e-02,\n",
       "                      -2.0428e-02, -3.4442e-03,  4.1138e-02,  5.5864e-02,  2.0824e-02,\n",
       "                      -1.1078e-02,  4.4847e-02,  3.8500e-02, -2.8101e-02, -7.0068e-02,\n",
       "                       8.5603e-02,  4.7706e-03,  5.5386e-02, -1.2429e-02, -1.0463e-02,\n",
       "                      -2.3856e-02,  4.5866e-02,  7.3448e-02, -2.0464e-02,  1.0335e-01,\n",
       "                       3.0597e-05,  1.0526e-02, -4.6046e-03,  8.1041e-02, -1.1131e-03,\n",
       "                       6.0180e-03,  5.0609e-02, -2.7753e-02,  1.4082e-02, -9.6786e-03,\n",
       "                      -2.4593e-02,  5.8305e-02, -2.4494e-03, -1.6714e-03,  2.0025e-03,\n",
       "                      -8.5828e-03, -3.2386e-02, -6.0944e-03,  4.2038e-02, -2.2595e-02,\n",
       "                      -2.1682e-02,  8.3651e-03, -8.9202e-02,  5.0481e-02, -8.5322e-04,\n",
       "                      -4.6111e-02,  7.8319e-02, -2.3758e-02,  1.1744e-02,  1.6127e-02,\n",
       "                       3.7687e-02,  8.2111e-02,  1.5358e-02,  5.2750e-02,  7.7587e-02,\n",
       "                       3.3775e-02,  3.1076e-02, -5.7350e-02, -2.6058e-03, -2.3924e-02,\n",
       "                       4.6959e-02,  3.1517e-02, -3.9790e-02, -3.3574e-02, -7.1657e-03,\n",
       "                      -4.1888e-03, -1.5299e-02, -5.3350e-02, -2.6358e-02, -2.1508e-02,\n",
       "                       2.5752e-02, -1.6808e-02, -2.0769e-02, -1.2932e-02, -2.8014e-02,\n",
       "                      -2.5591e-03, -3.7647e-02,  2.5476e-04,  7.4074e-03,  1.9451e-02,\n",
       "                      -1.0759e-02, -1.6435e-02,  8.7933e-03,  3.9947e-02,  1.9273e-02,\n",
       "                       1.0993e-02, -1.6116e-02,  4.2712e-02,  4.1387e-03,  7.5077e-02,\n",
       "                       9.5411e-02, -3.4230e-02,  1.5958e-03,  6.9340e-02,  3.6048e-03,\n",
       "                      -4.8047e-03,  2.6441e-02,  6.1005e-02, -2.4037e-02, -5.1848e-02,\n",
       "                      -2.8015e-02,  8.2514e-03,  4.3583e-02,  4.8212e-02,  3.2669e-02,\n",
       "                       3.0612e-02,  6.0211e-02,  9.3768e-02,  2.5735e-02,  3.7666e-02,\n",
       "                       9.0457e-02,  7.1851e-02, -5.2013e-03, -3.6567e-02, -1.8093e-02,\n",
       "                       3.1607e-02, -3.5706e-02, -8.8622e-03, -3.9072e-02,  6.6068e-02,\n",
       "                       4.9810e-03,  6.0553e-02, -3.9495e-02,  7.4693e-03, -7.9742e-03,\n",
       "                       6.3776e-02, -4.8873e-02, -7.7344e-02, -3.7590e-02,  1.6604e-02,\n",
       "                       8.6816e-02, -5.1582e-02, -4.1218e-02, -1.0295e-02,  2.3381e-02,\n",
       "                       6.9458e-02,  1.5693e-02, -4.0282e-03, -3.6038e-02,  5.2498e-02,\n",
       "                      -4.5274e-02,  2.4293e-02,  9.4074e-02,  1.4954e-02,  2.6398e-02,\n",
       "                       2.5546e-02,  5.7732e-02,  3.9236e-02, -1.0906e-03,  2.0823e-02,\n",
       "                       2.8954e-03,  5.3651e-02, -2.5991e-02, -4.1507e-02, -7.7266e-03,\n",
       "                       7.3137e-02, -3.9636e-02,  7.0103e-02, -3.2631e-02,  3.9533e-02,\n",
       "                      -2.3909e-02,  9.6065e-03, -1.8012e-02, -9.4363e-04, -3.5196e-02,\n",
       "                       1.4270e-02, -3.1637e-02, -4.2864e-02,  1.1151e-02,  3.0062e-02,\n",
       "                      -3.2578e-02,  3.4218e-02, -7.2345e-03,  6.5626e-02, -4.3555e-02,\n",
       "                      -1.8003e-02, -7.8424e-02,  2.1155e-02,  5.1012e-02,  7.5424e-03,\n",
       "                       1.1205e-01,  7.1403e-02, -5.2240e-03, -3.4398e-03,  6.2303e-02,\n",
       "                       5.1007e-02, -4.4719e-02, -1.4422e-03,  9.5461e-02,  7.7238e-05,\n",
       "                       9.8401e-02,  5.0135e-02,  4.8583e-02,  4.6180e-02,  9.4076e-03,\n",
       "                       4.8961e-02, -3.3605e-02, -1.8567e-02,  4.9197e-03,  6.0165e-02,\n",
       "                       2.5738e-02,  9.8033e-03,  1.4812e-03, -2.3482e-02, -4.4010e-02,\n",
       "                       2.7719e-02,  3.3170e-02,  3.4162e-02, -5.4210e-02,  2.6587e-02,\n",
       "                       2.0886e-03,  2.0562e-02, -2.5395e-03,  8.7869e-03,  3.1882e-02,\n",
       "                      -1.1156e-02, -2.8327e-02, -4.3461e-02, -4.2474e-02,  8.4179e-02,\n",
       "                      -4.2488e-02, -1.3131e-02, -7.7954e-02, -9.5427e-03, -6.8659e-02,\n",
       "                      -2.6618e-02,  5.8571e-02,  1.6932e-02, -2.0321e-02,  6.7421e-02,\n",
       "                      -4.0341e-04,  6.1543e-03,  1.8877e-02, -4.2212e-02,  6.0280e-02,\n",
       "                       2.7499e-02,  7.3925e-02], device='cuda:0')),\n",
       "             ('linear_relu_stack.4.weight',\n",
       "              tensor([[ 0.0196,  0.0717,  0.0315,  ...,  0.1126,  0.0314,  0.0867],\n",
       "                      [-0.0216, -0.0057,  0.1120,  ...,  0.0547,  0.0221,  0.1121],\n",
       "                      [-0.0349, -0.0614,  0.0100,  ..., -0.0877,  0.1124,  0.1003],\n",
       "                      ...,\n",
       "                      [ 0.0228, -0.2458, -0.0976,  ...,  0.0528, -0.0629,  0.1152],\n",
       "                      [ 0.0329,  0.0133, -0.0047,  ...,  0.0597,  0.0004,  0.0336],\n",
       "                      [ 0.0401, -0.1535, -0.0325,  ..., -0.1988, -0.0314, -0.1669]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear_relu_stack.4.bias',\n",
       "              tensor([ 0.0193, -0.1478,  0.0192,  0.0596, -0.1887,  0.5411,  0.0611,  0.1585,\n",
       "                      -0.1889, -0.2971], device='cuda:0'))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
